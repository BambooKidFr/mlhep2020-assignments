{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom\n",
    "from scipy.stats import norm\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import pyro\n",
    "import numpy.testing as np_testing\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from scipy.special import expit, logit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sns.set(font_scale=1.5, rc={'figure.figsize':(11.7, 8.27)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Seminar plan\n",
    "\n",
    "    1. Fit binomial distribution to the space worms teeth distribution using KL Divergence, Reverse KL Divergence and Jensen-Shannon Divergence;\n",
    "    2. Fit 2D-normal distribution;\n",
    "        - with KL Divergence;\n",
    "        - with rKL Divergence;\n",
    "    3. Feel frustration and surprise and learn the horrible truth about rKL;\n",
    "    4. Take a glance at what happens in GANs;\n",
    "        - fit another network to approximate computation of KL/rKL;\n",
    "    5. Apply new knowledge to fit 2D-normal distribution.\n",
    "    \n",
    "    \n",
    "###### Btw, f-divergences are classical statistical tool originally...\n",
    "\n",
    "![](https://miro.medium.com/max/1432/1*hXK4F_vFtG-fh2BrxDolFg.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Discrete example\n",
    "\n",
    "In this notebook we're going to take a look at a way of comparing two probability distributions called Kullback-Leibler Divergence. \n",
    "\n",
    "## 1.1 Space worms teeth distribution\n",
    "\n",
    "Suppose that we're space-scientists visiting a distant, new planet and we've discovered a species of biting worms that we'd like to study. We've found that these worms have 10 teeth, but because of all the chomping away, many of them end up missing teeth. After collecting many samples we have come to this empirical probability distribution of the number of teeth in each worm:\n",
    " \n",
    "![Space worms](https://images.squarespace-cdn.com/content/v1/54e50c15e4b058fc6806d068/1494401025139-ODE7CP2043TS1CO9MQSN/ke17ZwdGBToddI8pDm48kLuT3KTpMRZ2imBrzIWD9_5Zw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVG-_BClLJADi5Tjms1vR9XfE3ardhQXleMJTem2-1ZqRideLm3HbGNLisCtv4-dzhc/biting-worms.jpg?format=1000w)\n",
    "\n",
    "Picture and idea credits: https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the probability adds up to 1\n",
    "true_data = torch.tensor([0.1, 0.2, 0.11, 0.11, 0.05, 0.02, 0.03, 0.05, 0.11, 0.15, 0.07])\n",
    "n = true_data.shape[0]\n",
    "index = torch.arange(n).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_testing.assert_almost_equal(true_data.sum().item(), 1., err_msg='Your probabilities do not sum up to 1!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.bar(index, true_data)\n",
    "plt.xlabel('Teeth Number')\n",
    "plt.title('Probability Distribution of Space Worm Teeth (empirically speaking)')\n",
    "plt.ylabel('Probability')\n",
    "plt.xticks(index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Hypothesis about the data\n",
    "\n",
    "###### Hypothesis 1: uniform distribution of space worms teeth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_data = torch.full((n, ), 1.0 / n)\n",
    "\n",
    "plt.figure()\n",
    "# we can plot our approximated distribution against the original distribution\n",
    "width = 0.3\n",
    "plt.bar(index, true_data, width=width, label='True')\n",
    "plt.bar(index + width, uniform_data, width=width, label='Uniform')\n",
    "plt.xlabel('Teeth Number')\n",
    "plt.title('Probability Distribution of Space Worm Teeth')\n",
    "plt.ylabel('Probability')\n",
    "plt.xticks(index)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hypothesis 2: binomial distribution with unknown parameter $p$\n",
    "\n",
    "\n",
    "###### Introducing a new framework: Pyro\n",
    "Pyro is a flexible, scalable deep probabilistic programming library built on PyTorch. (c) https://github.com/pyro-ppl/pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro import distributions as distrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we estimate the parameter of the binomial distribution\n",
    "p = true_data.dot(index) / n\n",
    "print('p for binomial distribution:', p)\n",
    "binomial_dist = distrs.Binomial(total_count=n, probs=p)\n",
    "binom_data = binomial_dist.log_prob(index).exp()\n",
    "binom_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "width = 0.3\n",
    "plt.bar(index, true_data, width=width, label='True')\n",
    "plt.bar(index + width, binom_data, width=width, label='Binomial')\n",
    "plt.xlabel('Teeth Number')\n",
    "plt.title('Probability Distribution of Space Worm Teeth')\n",
    "plt.ylabel('Probability')\n",
    "plt.xticks(np.arange(n))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(index - width, true_data, width=width, label='True')\n",
    "plt.bar(index, uniform_data, width=width, label='Uniform')\n",
    "plt.bar(index + width, binom_data, width=width, label='Binomial')\n",
    "plt.xlabel('Teeth Number')\n",
    "plt.title('Probability Distribution of Space Worm Teeth Number')\n",
    "plt.ylabel('Probability')\n",
    "plt.xticks(index)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Task: Implementing KL Divergance for a discrete distribution\n",
    "\n",
    "$$KL ( P||Q ) = \\int_{R^n}p(x)\\log\\left( \\frac{p(x)}{q_{\\theta}(x)}\\right) dx$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68cdb78a470804aa8f4834ddd4d22f63",
     "grade": false,
     "grade_id": "cell-a35d39a04c4613fc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_kl_divergence(p_probs: torch.Tensor, q_probs: torch.Tensor):\n",
    "    \"\"\"\"\n",
    "    KL (P || Q) = \\int_{R^n}p(x)\\log\\left( \\frac{p(x)}{q_{\\theta}(x)}\\right) = \n",
    "                = \\sum_i p_i log(p_i / q_i)\n",
    "    \n",
    "    Note:\n",
    "        1. The output -- kl_div -- should be one number that is equal to KL (P || Q)\n",
    "        2. Do not forget to clamp your probabilities to avoid log(0) and (x / 0) problems!\n",
    "    \"\"\"\n",
    "    EPS = 1e-7\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    return kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8cdb2c09d570337500aac15eb296c6bc",
     "grade": true,
     "grade_id": "compute_kl_divergence",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "true_data_testing = torch.tensor([0.1, 0.2, 0.11, 0.11, 0.05, 0.02, 0.03, 0.05, 0.11, 0.15, 0.07])\n",
    "uniform_data_testing = torch.full((n, ), 1.0 / n)\n",
    "binomial_dist = distrs.Binomial(total_count=n, probs=true_data.dot(index) / n).log_prob(index).exp()\n",
    "\n",
    "\n",
    "print('KL(True||Uniform): ', compute_kl_divergence(true_data, uniform_data))\n",
    "np_testing.assert_almost_equal(compute_kl_divergence(true_data, uniform_data).item(), 0.16362181305885315, decimal=3)\n",
    "\n",
    "print('KL(Uniform||True): ', compute_kl_divergence(uniform_data, true_data))\n",
    "np_testing.assert_almost_equal(compute_kl_divergence(uniform_data, true_data).item(), 0.1930386871099472, decimal=3)\n",
    "\n",
    "\n",
    "print('KL(True||Binomial): ', compute_kl_divergence(true_data, binom_data))\n",
    "np_testing.assert_almost_equal(compute_kl_divergence(true_data, binom_data).item(), 1.5118370056152344, decimal=3)\n",
    "\n",
    "\n",
    "print('KL(Binomial||True): ', compute_kl_divergence(binom_data, true_data))\n",
    "np_testing.assert_almost_equal(compute_kl_divergence(binom_data, true_data).item(), 1.1197633743286133, decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Task: Implementing JS Divergance for a discrete distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d785e8c93be7403c1926d7d7648e2de",
     "grade": false,
     "grade_id": "cell-8bc3c2004c29de91",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_js_divergence(p_probs, q_probs):\n",
    "    \"\"\"\"\n",
    "    JS (P || Q) = (KL(P || Q) + KL(Q || P)) / 2\n",
    "    Note:\n",
    "        1. The output should be one number that is equal to KL (P || Q)\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    return (kl_div + rkl_div) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d26fd4a1b91ad96af4a36584072b248",
     "grade": true,
     "grade_id": "compute_js_divergence",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "true_data_testing = torch.tensor([0.1, 0.2, 0.11, 0.11, 0.05, 0.02, 0.03, 0.05, 0.11, 0.15, 0.07])\n",
    "uniform_data_testing = torch.full((n, ), 1.0 / n)\n",
    "binomial_dist = distrs.Binomial(total_count=n, probs=true_data.dot(index) / n).log_prob(index).exp()\n",
    "\n",
    "print('JS(True||Uniform): ', compute_js_divergence(true_data, uniform_data))\n",
    "np_testing.assert_almost_equal(compute_js_divergence(true_data, uniform_data).item(), 0.17833024263381958, decimal=3)\n",
    "\n",
    "print('JS(Uniform||True): ', compute_js_divergence(uniform_data, true_data))\n",
    "np_testing.assert_almost_equal(compute_js_divergence(uniform_data, true_data).item(), 0.17833024263381958, decimal=3)\n",
    "\n",
    "print('JS(True||Binomial): ', compute_js_divergence(true_data, binom_data))\n",
    "np_testing.assert_almost_equal(compute_js_divergence(true_data, binom_data).item(), 1.3158001899719238, decimal=3)\n",
    "\n",
    "print('JS(Binomial||True): ', compute_js_divergence(binom_data, true_data))\n",
    "np_testing.assert_almost_equal(compute_js_divergence(binom_data, true_data).item(), 1.3158001899719238, decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Implementing JS Divergance for a discrete distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Firslty, a few words about backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.tensor(0.5, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial_dist = distrs.Binomial(total_count=n, probs=p)\n",
    "binom_data = binomial_dist.log_prob(index).exp()\n",
    "binom_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice that now we have `grad_fn=<ExpBackward>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = compute_kl_divergence(true_data, binom_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because KL have  `grad_fn=<SumBackward0>` we can apply `.backward()` method to compute the gradients w.r.t. `p`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Task: Implementing optimization procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42b072f12d6aab196058896503904b9b",
     "grade": false,
     "grade_id": "cell-b5692602c921f1b9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "EPS = 1e-3\n",
    "2\n",
    "def optimize_binomial_distribution(func, p, true_data, lr: float=1e-3, epochs: int=1000):\n",
    "    \"\"\"\n",
    "    func: distance function that takes two arguments\n",
    "    p: initial guess on parameter p of binomial distribution\n",
    "    true_data: true_data\n",
    "    lr: learning rate\n",
    "    epochs: number of training iterations\n",
    "    \"\"\"\n",
    "    p = p.clone().detach().requires_grad_(True)\n",
    "    history = defaultdict(list)\n",
    "    opt = optim.Adam([p], lr=lr)\n",
    "    \n",
    "    n = len(true_data)\n",
    "    index = torch.arange(n).float()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        p.data.clamp_(EPS, 1 - EPS)\n",
    "        # Here your task is to \n",
    "        # 1. create binomial distribution with Pyro\n",
    "        # 2. calculate p.d.f. of the binomial distribution\n",
    "        # 3. apply func to true_data and p.d.f. from previous step\n",
    "        # 4. perform usual backprop\n",
    "        # Do not forget to zero grad after weights update!\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        history['epoch'].append(i)\n",
    "        history['dist'].append(d.item())\n",
    "        history['p'].append(p.item())\n",
    "\n",
    "    return p, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1 Task: Fitting with KL-Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_p, hist = optimize_binomial_distribution(compute_kl_divergence, p, true_data)\n",
    "print(optimal_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebec45a6ec72324e756879819c681095",
     "grade": true,
     "grade_id": "kl_div",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np_testing.assert_almost_equal(optimal_p.item(), 0.4099, decimal=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binom_data = distrs.Binomial(total_count=n, probs=optimal_p).log_prob(index).exp().detach()\n",
    "plt.figure()\n",
    "width = 0.3\n",
    "plt.bar(index, true_data, width=width, label='True')\n",
    "plt.bar(index + width, binom_data, width=width, label='Binomial fitted with KL')\n",
    "plt.xlabel('Teeth Number')\n",
    "plt.title('Probability Distribution of Space Worm Teeth')\n",
    "plt.ylabel('Probability')\n",
    "plt.xticks(np.arange(n))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2 Task: Fitting with rKL-Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_p, hist = optimize_binomial_distribution(lambda x, y: compute_kl_divergence(y, x), p, true_data)\n",
    "print(optimal_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b40217fad44cd4c81c289255650556e9",
     "grade": true,
     "grade_id": "rkl_div",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np_testing.assert_almost_equal(optimal_p.item(), 0.9973, decimal=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binom_data = distrs.Binomial(total_count=n, probs=optimal_p).log_prob(index).exp().detach()\n",
    "binom_data = binom_data / binom_data.sum()\n",
    "plt.figure()\n",
    "width = 0.3\n",
    "plt.bar(index, true_data, width=width, label='True')\n",
    "plt.bar(index + width, binom_data, width=width, label='Binomial fitted with rKL')\n",
    "plt.xlabel('Teeth Number')\n",
    "plt.title('Probability Distribution of Space Worm Teeth')\n",
    "plt.ylabel('Probability')\n",
    "plt.xticks(np.arange(n))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.3 Task: Fitting with JS-Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_p, hist = optimize_binomial_distribution(lambda x, y: compute_js_divergence(y, x), p, true_data)\n",
    "print(optimal_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f8491d4763034d76fe1db118bbe58a8",
     "grade": true,
     "grade_id": "ks_div",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np_testing.assert_almost_equal(optimal_p.item(), 0.3484, decimal=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binom_data = distrs.Binomial(total_count=n, probs=optimal_p).log_prob(index).exp().detach()\n",
    "plt.figure()\n",
    "width = 0.3\n",
    "plt.bar(index, true_data, width=width, label='True')\n",
    "plt.bar(index + width, binom_data, width=width, label='Binomial fitted with KL')\n",
    "plt.xlabel('Teeth Number')\n",
    "plt.title('Probability Distribution of Space Worm Teeth')\n",
    "plt.ylabel('Probability')\n",
    "plt.xticks(np.arange(n))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Compare divergence profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = torch.linspace(0. + EPS, 1. - EPS, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rkls = [\n",
    "    compute_kl_divergence(distrs.Binomial(total_count=n, probs=p).log_prob(index).exp(), true_data) for p in ps\n",
    "]\n",
    "\n",
    "kls = [\n",
    "    compute_kl_divergence(true_data, distrs.Binomial(total_count=n, probs=p).log_prob(index).exp()) for p in ps\n",
    "]\n",
    "\n",
    "jss = [\n",
    "    compute_js_divergence(distrs.Binomial(total_count=n, probs=p).log_prob(index).exp(), true_data) for p in ps\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(ps, rkls, label='rKL')\n",
    "plt.plot(ps, kls, label='KL')\n",
    "plt.plot(ps, jss, label='JS')\n",
    "plt.title('Probability Distribution of Space Worm Teeth')\n",
    "plt.ylabel('Teeth Number')\n",
    "plt.xlabel('Probability')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fitting 2D-normal distribution\n",
    "\n",
    "Credits: my collegues that https://github.com/HSE-LAMBDA/DeepGenerativeModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supplementary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import MultivariateNormal\n",
    "def plot_2d_dots(dots, color='blue', label='None'):\n",
    "    plt.ylim(-10, 10)\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.scatter(dots[:, 0], dots[:, 1], s=1, c=color, label=label)\n",
    "\n",
    "\n",
    "def get_parameters(mu=0., sigma=1.):\n",
    "    train_mu = torch.Tensor([mu, mu]).requires_grad_(True)\n",
    "    train_sigma = torch.Tensor([[sigma, 0.0],\n",
    "                                [0.0, sigma]]).requires_grad_(True)\n",
    "    return train_mu, train_sigma\n",
    "\n",
    "def create_distr(mu, sigma):\n",
    "    return distrs.MultivariateNormal(mu, sigma)\n",
    "\n",
    "\n",
    "def sample(d, num):\n",
    "    return d.sample(torch.Size([num]))\n",
    "\n",
    "class MixtureDistribution:\n",
    "    def __init__(self, p1, p2, w=0.5):\n",
    "        self._p1 = p1\n",
    "        self._p2 = p2\n",
    "        self._w = w\n",
    "        \n",
    "    def sample(self, n):\n",
    "        return torch.cat([sample(self._p1, int(n * self._w)), sample(self._p2, n - int(n * self._w))])\n",
    "    \n",
    "    def log_prob(self, x):\n",
    "        return (self._w * self._p1.log_prob(x).exp() + (1. - self._w) * self._p2.log_prob(x).exp()).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target distribution\n",
    "\n",
    "Target distribution is a mixture of two 2-dimensionals normal distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Generation\n",
    "\n",
    "Here we sampling training data from real distribution:\n",
    "\n",
    "\n",
    "$$D_{train} = \\{x\\}_{i=1}^{n} \\sim p(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = create_distr(\n",
    "    mu=torch.Tensor([-5, -5]), \n",
    "    sigma=torch.Tensor([[1., 0.0], \n",
    "                        [0.0, 1.]])\n",
    ")\n",
    "P2 = create_distr(\n",
    "    mu=torch.Tensor([4, 3]), \n",
    "    sigma=torch.Tensor([[1., 0.0], \n",
    "                        [0.0, 1.]])\n",
    ")\n",
    "\n",
    "P = MixtureDistribution(P1, P2, 0.5)\n",
    "\n",
    "samples_x = P.sample(2000)\n",
    "px = P.log_prob(samples_x).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_2d_dots(samples_x, color=px, label='Target distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mu, train_sigma = get_parameters()\n",
    "\n",
    "Q = create_distr(train_mu, train_sigma)\n",
    "samples_q = sample(Q, 1000)\n",
    "plt.figure()\n",
    "plot_2d_dots(samples_x, color='r', label='Target distribution: P(x)')\n",
    "plot_2d_dots(samples_q, color= Q.log_prob(samples_q).exp().detach(), label='Search distribution: Q(x)')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Task: optimization of KL\n",
    "\n",
    "In cases when the only thing you have is the data sampled from real distribution:\n",
    "\n",
    "$$D_{train} = \\{x\\}_{i=1}^{n} \\sim p(x)$$\n",
    "\n",
    "To estimate KL divergance you can apply MC estimation. I.e.\n",
    "\n",
    "\n",
    "$$KL ( P||Q ) = \\int_{R^n}p(x)\\log\\left( \\frac{p(x)}{q_{\\theta}(x)}\\right) dx \\approx \\sum\\limits_{x \\in D} p(x) \\log\\left( \\frac{p(x)}{q(x)} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22af0df1132b8a41ea43b669eca58f41",
     "grade": false,
     "grade_id": "cell-4379f4858008e6df",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def estimate_kl(samples_x, px, train_mu, train_sigma, P):\n",
    "    \"\"\"\n",
    "    Here your task is to estimate KL divergance\n",
    "    \n",
    "    1. Create Q distribution with parameters train_mu and train_sigma\n",
    "    2. Estimate qx on samples from real distribution\n",
    "    3. apply function for KL computation\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "576e60f88c763ff48840ceb267606f6b",
     "grade": true,
     "grade_id": "estimate_kl",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_mu, train_sigma = get_parameters(mu=0., sigma=1.)\n",
    "samples_x_test = torch.linspace(-5, 5, 10).view(-1, 2)\n",
    "px_test = P.log_prob(samples_x_test).exp()\n",
    "np_testing.assert_almost_equal(estimate_kl(samples_x_test, px_test, train_mu, train_sigma, P).item(), 0.689806, decimal=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee84d54952efd01e18911c9bfde11dab",
     "grade": false,
     "grade_id": "cell-b591c4929ab84b83",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_mu, train_sigma = get_parameters(mu=0., sigma=1.)\n",
    "optim = torch.optim.Adam([train_mu, train_sigma], lr=1e-2)\n",
    "\n",
    "for i in range(5000):\n",
    "    # Here your task is to \n",
    "    # 1. create Q distribution with \n",
    "    # 2. calculate p.d.f. of the binomial distribution\n",
    "    # 3. apply func to true_data and p.d.f. from previous step\n",
    "    # 4. perform usual backprop\n",
    "    # Do not forget to zero grad after weights update!\n",
    "\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    if i % 200 == 0:\n",
    "        # plot pdfs\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.title(f'KL={loss.item()}, iter={i}')\n",
    "        plot_2d_dots(samples_x, color='r', label='Target distribution: P(x)')\n",
    "        samples_q = sample(Q, 1000)\n",
    "        plot_2d_dots(samples_q.detach(), color= Q.log_prob(samples_q).exp().detach(), label='Search distribution: Q(x)')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check: \n",
    "\n",
    "The search distribution should look like its trying to cover both modes of the target distribution. Like in the lecture :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Task: optimization of rKL\n",
    "\n",
    "###### Here you will understand: \n",
    "    - the hidden power of Pyro\n",
    "    - what is wrong with rKL\n",
    "    \n",
    "    \n",
    "    \n",
    "To estimate rKL divergance you can apply MC estimation. I.e.\n",
    "\n",
    "\n",
    "$$KL ( P||Q ) = \\int_{R^n} q_{\\theta}(x) \\log\\left( \\frac{q_{\\theta}(x)}{p(x)}\\right) dx \\approx \\sum\\limits_{x \\in q(x)} q_{\\theta}(x) \\log\\left( \\frac{q_{\\theta}(x)}{p(x)} \\right)$$\n",
    "\n",
    "###### Note that here we are averaging over samples from q(x)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mu, train_sigma = get_parameters(mu=0., sigma=1.)\n",
    "Q = create_distr(train_mu, train_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample(Q, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(d, num):\n",
    "    \"\"\"\n",
    "    Sample from distribution num samples with __Pyro__\n",
    "    \"\"\"\n",
    "    res = pyro.sample(\"dist\", d.expand([num]))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mu, train_sigma = get_parameters(mu=0., sigma=1.)\n",
    "Q = create_distr(train_mu, train_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample(Q, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22e6c3dd4ba7faac545444f5f8f2c3c9",
     "grade": false,
     "grade_id": "cell-e65947956a9a9813",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def estimate_reverse_kl(samples_x, train_mu, train_sigma, P, n_samples=2000):\n",
    "    \"\"\"\n",
    "    Here your task is to estimate rKL divergance\n",
    "    \n",
    "    1. Create Q distribution with parameters train_mu and train_sigma\n",
    "    2. Sample(!) points x_q from Q distribution\n",
    "    3. Estimate p(x_q)\n",
    "    4. Estimate q(x_q)\n",
    "    3. apply function for KL computation\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c1a32955c2f1331cb8131940b154c18",
     "grade": true,
     "grade_id": "estimate_reverse_kl",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_mu, train_sigma = get_parameters(mu=0., sigma=1.)\n",
    "samples_x_test = torch.linspace(-5, 5, 10).view(-1, 2)\n",
    "px_test = P.log_prob(samples_x_test).exp()\n",
    "torch.manual_seed(0)\n",
    "np_testing.assert_almost_equal(estimate_reverse_kl(samples_x_test, train_mu, train_sigma, P, 2000).item(), 1965.1910400390625, decimal=3)\n",
    "torch.manual_seed(np.random.randint(low=0, high=10000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mu, train_sigma = get_parameters(mu=0.0, sigma=1.)\n",
    "\n",
    "optim = torch.optim.Adam([train_mu, train_sigma], lr=1e-2)\n",
    "\n",
    "for i in range(5000):\n",
    "    optim.zero_grad()\n",
    "    Q = create_distr(train_mu, train_sigma)\n",
    "    loss = estimate_reverse_kl(samples_x, train_mu, train_sigma, P)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if i % 200 == 0:\n",
    "        # plot pdfs\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.title(f'KL={loss.item()}, iter={i}')\n",
    "        plot_2d_dots(samples_x, color='r', label='Target distribution: P(x)')\n",
    "        samples_q = sample(Q, 1000)\n",
    "        plot_2d_dots(samples_q.detach(), color=Q.log_prob(samples_q).exp().detach(), label='Search distribution: Q(x)')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check: \n",
    "\n",
    "The search distribution should look like its trying to cover one mode of the target distribution. Like in the lecture :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Breaking truth about rKL, many tears\n",
    "\n",
    "_[here i will speak some truth]_\n",
    "\n",
    "\n",
    "![](https://pbs.twimg.com/media/EUXNKoxXkAAgUQ4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's go back to the square one and take a look on 1-dimensional normal distribution\n",
    "\n",
    "Credits for some parts of code below: http://louistiao.me/notes/a-simple-illustration-of-density-ratio-estimation-and-kl-divergence-estimation-by-probabilistic-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_p = torch.tensor(1.)\n",
    "sigma_p = torch.tensor(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_q = torch.tensor(0.)\n",
    "sigma_q = torch.tensor(2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = distrs.Normal(mu_p, sigma_p)\n",
    "Q = distrs.Normal(mu_q, sigma_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = torch.tensor(np.linspace(-5., 5., 500)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title('probability densities')\n",
    "\n",
    "ax.plot(xs, Q.log_prob(xs).exp(), label='$q(x)$')\n",
    "ax.plot(xs, P.log_prob(xs).exp(), label='$p(x)$')\n",
    "\n",
    "ax.set_xlim(-5.5, 5.5)\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('density')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Density Ratio\n",
    "\n",
    "The ratio of their probability densities is given by,\n",
    "\n",
    "$$\n",
    "r(x) = \\frac{p(x)}{q(x)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b394012ab0c535c1e8de5e1c0dad7392",
     "grade": false,
     "grade_id": "cell-4904614af2b288b8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ae38b409ee83179e4d46a5e2c980ca5",
     "grade": true,
     "grade_id": "density_ratio",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np_testing.assert_almost_equal(\n",
    "    density_ratio(torch.tensor([1., 2., 3.]), P, Q).numpy(),\n",
    "    np.array([2.266297 , 2.0000002, 0.833724]),\n",
    "    decimal=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title('Density ratio')\n",
    "\n",
    "ax.plot(xs, density_ratio(xs, P, Q))\n",
    "\n",
    "ax.set_xlim(-5.5, 5.5)\n",
    "ax.set_xlabel('$x$')\n",
    "\n",
    "ax.set_ylabel('$r(x)$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title('log density ratio')\n",
    "\n",
    "ax.plot(xs, np.log(density_ratio(xs, P, Q)))\n",
    "\n",
    "ax.set_xlim(-5.5, 5.5)\n",
    "ax.set_xlabel('$x$')\n",
    "\n",
    "ax.set_ylabel('$r(x)$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Density ratio estimation by probabilistic classification\n",
    "\n",
    "Suppose now that we don't have access to the densities $p(x)$ and $q(x)$ for whatever reason, which is quite a common scenario. \n",
    "\n",
    "Density ratio estimation is concerned with directly estimating $r(x)$ using only samples from these distributions.One of the most simple approaches is __probabilistic classification__.\n",
    "\n",
    "---\n",
    "\n",
    "Let $\\mathcal{D}_p = \\{x_p^{(i)}\\}_{i=1}^{n_p}$ and $\\mathcal{D}_q = \\{x_q^{(j)}\\}_{j=1}^{n_q}$ be sets of samples drawn from distributions $p(x)$ and $q(x)$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_p = n_q = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_p = sample(P, n_p)\n",
    "samples_q = sample(Q, n_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat([samples_p, samples_q]).view(-1, 1)\n",
    "y = torch.cat([torch.ones_like(samples_p), \n",
    "               torch.zeros_like(samples_q)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=.2, random_state=1337\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.scatter(x_test, y_test, c=y_test, s=10.**2, \n",
    "           marker='s', alpha=.2, cmap='coolwarm_r')\n",
    "\n",
    "plt.xlim(-5.5, 5.5)\n",
    "\n",
    "plt.xlabel('samples')\n",
    "plt.yticks([0, 1], ['$q(x)$', '$p(x)$'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Bayes optimal classifier\n",
    "\n",
    "The Bayes optimal classifier can be, __by definition__ written as a function of the density ratio,\n",
    "\n",
    "$$\n",
    "p(y=1 \\mid x) = \\sigma(\\log r(x)) = \\frac{p(x)}{p(x) + q(x)},\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is a sigmoid function.\n",
    "\n",
    "We do not have an access to $p(x)$ and $q(x)$, thus we are building a classifier: $D_{\\theta}(x) := \\hat{p}(y = 1 \\mid x)$. $D_{\\theta}(x)$ could be a neural network any other classifier.\n",
    "\n",
    "Thus, if classifier it well trained then $\\hat{p}(y = 1 \\mid x) \\approx p(y = 1 \\mid x)$.\n",
    "\n",
    "Then ratio density for object $x$ could estimated via the __predicted probability by the trained classifer__ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{r}(x)   = \\frac{D(x)}{1 - D(x)}\n",
    "           & = \\exp \\left ( \\log \\frac{D(x)}{1 - D(x)} \\right ) \\\\\n",
    "           & = \\exp \\left ( \\sigma^{-1}(D(x)) \\right ). \\\\\n",
    "\\end{align}                               \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a016e1cf74c9cc211fc5e843276c3c73",
     "grade": false,
     "grade_id": "cell-622a8101d0c494bd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3153600a9a80d947c7c6f2b7a67acd88",
     "grade": true,
     "grade_id": "classifier_optimal",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np_testing.assert_almost_equal(\n",
    "    classifier_optimal(torch.tensor([1., 2., 3.]), P, Q).numpy(),\n",
    "    np.array([0.6938429 , 0.6666667 , 0.45466167]),\n",
    "    decimal=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_optimal = classifier_optimal(x_test, P, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title('Optimal class probabilities')\n",
    "\n",
    "ax.scatter(x_test, y_test, c=y_test, s=10.**2, \n",
    "           marker='s', alpha=.2, cmap='coolwarm_r')\n",
    "\n",
    "ax.scatter(x_test, y_pred_optimal, c=y_pred_optimal, cmap='coolwarm_r')\n",
    "\n",
    "ax.set_xlim(-5.5, 5.5)\n",
    "ax.set_xlabel('samples')\n",
    "\n",
    "ax.set_yticks([0., .5, 1.])\n",
    "ax.set_ylabel('$p(y=1|x)$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "critic = nn.Sequential(\n",
    "    nn.Linear(1, 16),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(16, 16),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(16, 1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "\n",
    "optim = torch.optim.Adam(critic.parameters(), lr=1e-2)\n",
    "ce = torch.nn.BCELoss()\n",
    "\n",
    "hist = defaultdict(list)\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    optim.zero_grad()\n",
    "    probs = critic(x_train)\n",
    "    loss = ce(probs.view(-1), y_train)\n",
    "    probs = critic(x_test)\n",
    "    loss_test = ce(probs.view(-1), y_test)\n",
    "    hist['loss_train'].append(loss.item())\n",
    "    hist['loss_test'].append(loss_test.item())\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot(hist['loss_train'], label='Train')\n",
    "plt.plot(hist['loss_test'], label='Test')\n",
    "\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_xlabel('epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_optimal = classifier_optimal(xs, P, Q)\n",
    "ys = critic(xs.view(-1, 1)).view(-1).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title('Class probabilities')\n",
    "\n",
    "ax.plot(xs, ys_optimal, label='Optimal')\n",
    "ax.plot(xs, ys, label='Estimated')\n",
    "\n",
    "ax.set_xlim(-5.5, 5.5)\n",
    "ax.set_xlabel('$x$')\n",
    "\n",
    "ax.set_ylim(0., 1.)\n",
    "ax.set_ylabel('$p(y=1|x)$')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Density ratio estimator\n",
    "\n",
    "$$r = \\mathrm{logit}(D(x))$$\n",
    "\n",
    "Task is to implement `log_odds_fn(x, critic)` that implements $r$-calculation, i.e. (1) prediction, (2) logit-computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec3f84079ffd6dc3b231da490a8afc0b",
     "grade": false,
     "grade_id": "cell-437b2643bf647aa5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9cd670fc23e37a60566b722463fa0ba",
     "grade": true,
     "grade_id": "log_odds_fn",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummpy_critic = nn.Sequential(\n",
    "    nn.Linear(1, 10),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(10, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        m.weight.data.fill_(0.1)\n",
    "        m.bias.data.fill_(0.1)\n",
    "dummpy_critic.apply(init_weights)\n",
    "np_testing.assert_almost_equal(\n",
    "    log_odds_fn(torch.tensor([0., 1., 2.]), dummpy_critic).detach().numpy(),\n",
    "    np.array([0.2  , 0.297, 0.391]),\n",
    "    decimal=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_odds = log_odds_fn(xs, critic).detach()\n",
    "log_density_ratios = np.log(density_ratio(xs, P, Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title('Log density ratio')\n",
    "\n",
    "ax.plot(xs, log_density_ratios, label='Analytical')\n",
    "ax.plot(xs, log_odds, label='Estimated')\n",
    "\n",
    "ax.set_xlim(-5.5, 5.5)\n",
    "ax.set_xlabel('$x$')\n",
    "\n",
    "ax.set_ylabel('$\\log r(x)$')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title('Density ratio')\n",
    "\n",
    "ax.plot(xs, density_ratio(xs, P, Q), label='Analytical')\n",
    "ax.plot(xs, np.exp(log_odds), label='Estimated')\n",
    "\n",
    "ax.set_xlim(-5.5, 5.5)\n",
    "ax.set_xlabel('$x$')\n",
    "\n",
    "ax.set_ylabel('$r(x)$')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 KL divergence estimation\n",
    "\n",
    "A natural application of density ratio estimation is divergence estimation. Namely, approximating a divergence from the general family of $f$-divergences using only samples. Here, we will approximate the Kullback-Liebler (KL) divergence between $P$ and $Q$ without using their respective densities, $p(x)$ and $q(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_samples = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_p = sample(P, mc_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_mc = pd.Series(density_ratio(samples_p, P, Q).log())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_mc_dre = log_odds_fn(samples_p, critic).detach()\n",
    "kl_mc_dre = pd.Series(kl_mc_dre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_mc_dre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_estimates = pd.concat([kl_mc, kl_mc_dre], axis=1, \n",
    "                         keys=['kl_mc', 'kl_mc_dre'])\n",
    "kl_estimates.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KL divergence between Gaussians\n",
    "\n",
    "For Gaussian distributions, the KL divergence can be evaluated analytically as,\n",
    "\n",
    "$$\n",
    "\\log \\sigma_q - \\log \\sigma_p - \\frac{1}{2} \n",
    "    \\left \\{ 1 - \\left (\\frac{\\sigma_p^2 + (\\mu_p - \\mu_q)^2}{\\sigma_q^2} \\right ) \\right \\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_gaussians(mu_p, sigma_p, mu_q, sigma_q):\n",
    "    r = mu_p - mu_q\n",
    "    return (np.log(sigma_q) - np.log(sigma_p) - .5 * (1. - (sigma_p**2 + r**2) / sigma_q**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_analytical = kl_divergence_gaussians(mu_p, sigma_p, mu_q, sigma_q)\n",
    "kl_analytical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.distplot(kl_estimates.kl_mc, ax=ax, label='Exact log density ratio')\n",
    "sns.distplot(kl_estimates.kl_mc_dre, ax=ax,  \n",
    "             label='Estimated log density ratio')\n",
    "\n",
    "ax.axvline(x=kl_analytical, color='r', linewidth=2., \n",
    "           label='KL Analytical')\n",
    "\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_xlabel('log density ratio')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.boxplot(x='estimator', y='sample',\n",
    "            data=pd.melt(kl_estimates, \n",
    "                         var_name='estimator', \n",
    "                         value_name='sample'), ax=ax)\n",
    "\n",
    "ax.axhline(y=kl_analytical, color='r', linewidth=2., \n",
    "           label='Analytical')\n",
    "\n",
    "ax.set_ylim(-1, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative mean of MC samples\n",
    "kl_estimates_cum_mean = kl_estimates.expanding().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "kl_estimates_cum_mean.plot(ax=ax, y='kl_mc', \n",
    "                           label='MC Estimate (Exact)')\n",
    "\n",
    "kl_estimates_cum_mean.plot(ax=ax, y='kl_mc_dre', \n",
    "                           label='MC Estimate (DRE)', )\n",
    "\n",
    "ax.axhline(y=kl_analytical, color='r', linewidth=2., \n",
    "           label='Analytical')\n",
    "\n",
    "ax.set_xlabel('Monte Carlo samples')\n",
    "ax.set_ylabel('KL Divergence')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Example of how to combine density ratio estimation and rKL optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = create_distr(\n",
    "    mu=torch.Tensor([-5, -5]), \n",
    "    sigma=torch.Tensor([[1., 0.0], \n",
    "                        [0.0, 1.]])\n",
    ")\n",
    "P2 = create_distr(\n",
    "    mu=torch.Tensor([4, 3]), \n",
    "    sigma=torch.Tensor([[1., 0.0], \n",
    "                        [0.0, 1.]])\n",
    ")\n",
    "\n",
    "P = MixtureDistribution(P1, P2, 0.5)\n",
    "\n",
    "samples_p = P.sample(2000)\n",
    "px = P.log_prob(samples_x).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_2d_dots(samples_p, color=px, label='Target distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mu, train_sigma = get_parameters()\n",
    "\n",
    "Q = create_distr(train_mu, train_sigma)\n",
    "samples_q = sample(Q, 1000)\n",
    "plt.figure()\n",
    "plot_2d_dots(samples_p, color='r', label='Target distribution: P(x)')\n",
    "plot_2d_dots(samples_q.detach(), color= Q.log_prob(samples_q).exp().detach(), label='Search distribution: Q(x)')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_critic(x, y, input_dim=2, epochs=50):\n",
    "    \"\"\"\n",
    "    This function create and train critic network with weights clipping\n",
    "    \"\"\"\n",
    "    critic = nn.Sequential(\n",
    "        nn.Linear(input_dim, 32),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(32, 32),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(32, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    critic_optim = torch.optim.Adam(critic.parameters(), lr=1e-3)\n",
    "    ce = torch.nn.BCELoss()\n",
    "    for _ in range(epochs):\n",
    "        critic_optim.zero_grad()\n",
    "        preds = critic(x).view(-1)\n",
    "        loss = ce(preds, y)\n",
    "        loss.backward()\n",
    "        critic_optim.step()\n",
    "    return critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate Reverse KL Divergence\n",
    "\n",
    "\\begin{equation*}\n",
    " KL ( q_{\\theta}||p  ) = \\int_{R^n}q_{\\theta}(x)\\log\\left( \\frac{q_{\\theta}(x)}{p(x)}\\right) dx. \n",
    "\\end{equation*}\n",
    "\n",
    "So now I am interested in $r_{reverse} = \\frac{q(x)}{p(x)}$.\n",
    "\n",
    "To estimate it, we need sligtly modify formula from above:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{r_{reverse}}(x) = \\exp \\left ( \\sigma^{-1}(1 - D(x)) \\right ). \\\\\n",
    "\\end{align}                               \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mu, train_sigma = get_parameters(mu=0.0, sigma=1.)\n",
    "train_sigma = train_sigma.requires_grad_(False)\n",
    "\n",
    "optim = torch.optim.SGD([train_mu, train_sigma], lr=10.)\n",
    "\n",
    "for i in range(200):\n",
    "    \n",
    "    Q = create_distr(train_mu, train_sigma)\n",
    "    samples_q = sample(Q, n_q).detach()\n",
    "    x = torch.cat([samples_p, samples_q])\n",
    "    y = torch.cat([torch.ones(len(samples_p)), \n",
    "                   torch.zeros(len(samples_q))])\n",
    "    critic = create_critic(x, y)\n",
    "    \n",
    "    samples_q = sample(Q, n_q)\n",
    "    critic(samples_q)\n",
    "    optim.zero_grad()\n",
    "    loss = (logit(1. - critic(samples_q))).view(-1).mean()\n",
    "    loss.backward()\n",
    "    \n",
    "    optim.step()\n",
    "    if i % 1 == 0:\n",
    "        # plot pdfs\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.title(f'KL={loss.item()}, iter={i}')\n",
    "        plot_2d_dots(x, color='r', label='Target distribution: P(x)')\n",
    "        samples_q = sample(Q, 1000)\n",
    "        plot_2d_dots(samples_q.detach(), color=Q.log_prob(samples_q).exp().detach(), label='Search distribution: Q(x)')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many problem, much pain\n",
    "    1. Convergance issue, thats why I fixed `train_sigma`\n",
    "    2. Slow convergance, thats why I am using lr=10\n",
    "    3. Not reliable estimation of density ration\n",
    "        - what if I overfitted?\n",
    "        - what if I underfitted?\n",
    "    4. Sometimes to get nice results you need to _restart training multiple times_\n",
    "        - the same is applied to GANs!\n",
    "\n",
    "\n",
    "![](https://i.redd.it/wvh152fq18c31.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
