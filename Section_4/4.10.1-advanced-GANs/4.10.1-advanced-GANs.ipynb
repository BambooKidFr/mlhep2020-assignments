{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Plan\n",
    "## This notebook\n",
    "1. Super-quick intro into BaBar DIRC subdetector\n",
    "2. You create a WGAP-GP GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "___Congradulations on making it to level two of GAN practice!___\n",
    "\n",
    "Here is a bonus: a relaxing link to play with advanced GANs without writing code:\n",
    "* https://affinelayer.com/pixsrv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# BaBar DIRC\n",
    "![DIRC scheme](https://www.slac.stanford.edu/BFROOT/www/Detector/DIRC/Gifs/NewDirc.gif)\n",
    "\n",
    "Detection of Internally Reflected Cherenkov light - a particle identification detector in other words. Unlike the calorimeter GANs, we will only generate high-level observables (particle delta log-likelihoods (DLL)) that are obtained after the reconstruction.\n",
    "\n",
    "We want the generation to be conditional on full kinematics of event: energy, pseudorapidity and the distance between the particle track and DIRC bar side of signal particle. For the data-driven GAN this is just 11-D tabular data.\n",
    "\n",
    "The data were generated with [FastDIRC](https://github.com/jmhardin/FastDIRC) modified to simulate two particle-events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 1 (difficulty: a couple of years ago this would have been a [paper](https://www.sciencedirect.com/science/article/pii/S0168900219300701) in a good journal, now should be doable in 10-40 minutes*)\n",
    "\\* after Denis Derkach formulated the problem and I did all the dirty work of getting training data\n",
    "\n",
    "Create a conditional Jensen-Shannon GAN to generate X given Y\n",
    "\n",
    "### Task 2 (difficulty: ~5 lines of code after completing task 1)\n",
    "\n",
    "Create a conditional classical Wasserstein GAN with gradient penalty to generate X given Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "\n",
    "URL_RE = r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "url_scrabber = re.compile(r'\\s*<input type=\"hidden\" name=\"downloadURL\" '\n",
    "                          'value=\"(?P<download_url>' + URL_RE + ')\" '\n",
    "                          'id=\"downloadURL\">')\n",
    "\n",
    "def get_cernbox_direct_link(url):\n",
    "    guard_page = requests.get(url)\n",
    "    for line in guard_page.text.split('\\n'):\n",
    "        match = url_scrabber.match(line)\n",
    "        if match:\n",
    "            return match.group('download_url')\n",
    "    raise RuntimeError(\"downloadURL not found. The most likely case is a change in CERNBox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual the data is availbale in CoCalc. But if it is not, here is the link\n",
    "# data_url = get_cernbox_direct_link(\"https://cernbox.cern.ch/index.php/s/hWCh4umYQ0KShjW\")\n",
    "DATA_FILE_NAME = '/home/user/share/data/4.10.1-advanced-GANs/kaons.hdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget \"$data_url\" -O \"$DATA_FILE_NAME\" -nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp, kstest\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_hdf(DATA_FILE_NAME)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_COLUMNS = [\"particle_one_energy\",\n",
    "             \"particle_two_energy\",\n",
    "             \"particle_one_eta\",\n",
    "             \"particle_two_eta\",\n",
    "             \"particle_one_x\",\n",
    "             \"particle_two_x\"]\n",
    "X_COLUMNS = [\"dll_electron\",\n",
    "             \"dll_kaon\",\n",
    "             \"dll_muon\",\n",
    "             \"dll_proton\",\n",
    "             \"dll_bt\"]\n",
    "assert X_COLUMNS + Y_COLUMNS == list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If feel physically inclined, feel free to make a hundred of plots.\n",
    "# However, for purposes of this exercise, abstract tabular X, Y will suffice.\n",
    "# In order to achive this wonderful state, the data needs to be normalized.\n",
    "plt.hist(data.values[:, 0], bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50958cdb99054bc33f07be264ca34b04",
     "grade": false,
     "grade_id": "6b1bab",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please use the sklearn.preprocessing.QuantileTransformer to transform each feature\n",
    "# into a Gaussian. Since it is just a normalisation, the exact shape is not important,\n",
    "# but there is a sanity check test a few cells below.\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "# Will take a couple of minutes\n",
    "# Please use the data_transformed variable to store the transfored dataset\n",
    "# data_transformed = <...>\n",
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After the transformation, features should look like this:\n",
    "![feature after normalization](https://github.com/yandexdataschool/mlhep2019/raw/master/notebooks/day-6/gauss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data_transformed[:, 0], bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "454edf7e6b457b7e8b91ec8e685e789b",
     "grade": true,
     "grade_id": "e6d15d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert data_transformed.shape == data.shape\n",
    "for feature in range(data_transformed.shape[1]):\n",
    "    assert kstest(data_transformed[:, feature], 'norm').statistic < 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data_transformed, test_size=0.1, random_state=124124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Please create a fully-connected generator. Remember, we need a conditional GAN, so the generator input should have dimensions for Y and noise; output - for X.\n",
    "\n",
    "You can start with a small one, make sure the code works, then add more layers. Having 3 hidden layer with 64 neurons in each should suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2fb6d935a6688df26f55ebe232fd9684",
     "grade": false,
     "grade_id": "ccfe09",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Note that WGAN does not work well with ELU (as per my experience and the original paper)\n",
    "# gen = <..>\n",
    "# your code here\n",
    "raise NotImplementedError\n",
    "gen_opt = torch.optim.RMSprop(gen.parameters(), lr=1e-2)\n",
    "gen_scheduler = torch.optim.lr_scheduler.MultiStepLR(gen_opt, milestones=[10000, 30000], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf3dab323ba792cb188b0d043ade25b1",
     "grade": true,
     "grade_id": "e8e847",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert gen[0].in_features == LATENT_DIM + len(Y_COLUMNS)\n",
    "assert gen[-1].out_features == len(X_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Please create a fully-connected discriminator. You can start with a small one, make sure the code works, then add more layers. Having 3 hidden layer with 64 neurons in each should suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "082f8a9dbacd21fdeca49c996679b2aa",
     "grade": false,
     "grade_id": "6adfae",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert disc[0].in_features == data.shape[1]\n",
    "assert disc[-1].out_features == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 1024\n",
    "def sample_real_data(batch_size):\n",
    "  \"\"\"\n",
    "  Inifintly repeats and shuffles the train dataset, outputs the\n",
    "  result in delicious batches.\n",
    "  \"\"\"\n",
    "  while True:\n",
    "    dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size,\n",
    "                                             shuffle=True, drop_last=True)\n",
    "    for batch in dataloader:\n",
    "        yield batch\n",
    "infinite_data = sample_real_data(TRAIN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! т"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Write a function that would sample the generator for given Y. The function should return a GPU-based tensor.\n",
    "\n",
    "`concatenate(generator(concatenate(noise, y)), y)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4618de8a012efb386b31b50aed88728c",
     "grade": false,
     "grade_id": "d42f29",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# sample_gen_data(y)\n",
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = sample_gen_data(torch.from_numpy(train[:3, len(X_COLUMNS):]).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46baebebe0a59c156ec99cc38e85ce93",
     "grade": true,
     "grade_id": "2d3697",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert test_sample.shape == (3, data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/caogang/wgan-gp/blob/master/gan_mnist.py\n",
    "def calc_gradient_penalty_classic(critic:torch.nn.Module,\n",
    "                                  real_data:torch.Tensor,\n",
    "                                  fake_data:torch.Tensor):\n",
    "    \"\"\"\n",
    "    Computes Gradient Penalty in random interpolates, in its classic form:\n",
    "    (|∇(D(x)|^2 - 1)^2, x is interpolated between a real and a generated sample\n",
    "    Args:\n",
    "    critic: a torch model whose gradient needs to be penalised\n",
    "    real_data[batch_size, n_features]: a sample of real data\n",
    "    fakse_data[batch_size, n_features]: a sample of fake data\n",
    "    Returns:\n",
    "    torch.Tensor, scalar, gradient penalty evalute\n",
    "    \"\"\"\n",
    "    assert real_data.shape == fake_data.shape\n",
    "    alpha = torch.rand(real_data.shape[0], 1)\n",
    "    alpha = alpha.expand(real_data.size()).cuda()\n",
    "\n",
    "    interpolates = (alpha * real_data + ((1 - alpha) * fake_data)).cuda()\n",
    "    interpolates = torch.autograd.Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = critic(interpolates)\n",
    "\n",
    "    gradients = torch.autograd.grad(\n",
    "      outputs=disc_interpolates, inputs=interpolates,\n",
    "      grad_outputs=torch.ones(disc_interpolates.size(), device=\"cuda\"),\n",
    "      create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 1.\n",
    "def train_disc(gan_type=\"WGAN-GP\"):\n",
    "    \"\"\"\n",
    "    Trains the discriminator for one step. Please note\n",
    "    this is not a pure function, it captutes the majority of variables\n",
    "    from the context.\n",
    "    \"\"\"\n",
    "    real_data = next(infinite_data).to(\"cuda\")\n",
    "    y_gen = next(infinite_data)[:, len(X_COLUMNS):].to(\"cuda\")\n",
    "    gen_data = sample_gen_data(y_gen)\n",
    "\n",
    "    if gan_type == \"JS\":\n",
    "        logp_real_is_real = F.logsigmoid(disc(real_data))\n",
    "        logp_gen_is_fake = F.logsigmoid(-disc(gen_data))\n",
    "        disc_loss = -logp_real_is_real.mean() - logp_gen_is_fake.mean()\n",
    "    elif gan_type == \"WGAN-GP\":\n",
    "        disc_loss = disc(real_data).mean() - disc(gen_data).mean() + \\\n",
    "                  calc_gradient_penalty_classic(disc, real_data, gen_data)*LAMBDA\n",
    "    else:\n",
    "        raise ValueError(\"Unknown GAN type. Valid ones are: JS, WGAN-GP\")\n",
    "\n",
    "    disc_opt.zero_grad()\n",
    "    disc_loss.backward()\n",
    "    disc_opt.step()\n",
    "    return disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6dcd4bfdd14e305c9ea613de47bed8f5",
     "grade": false,
     "grade_id": "fc320f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_gen(gan_type=\"WGAN-GP\"):\n",
    "    \"\"\"Trains generator for one step\"\"\"\n",
    "    real_data_y = next(infinite_data)[:, len(X_COLUMNS):].to(\"cuda\")\n",
    "    gen_data = sample_gen_data(real_data_y)\n",
    "\n",
    "    if gan_type == \"JS\":\n",
    "        # gen_loss = <your code here>\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "    elif gan_type == \"WGAN-GP\":\n",
    "        # gen_loss = <your code here>\n",
    "        ### BEIGN SOLUTION\n",
    "        gen_loss = disc(gen_data).mean()\n",
    "\n",
    "    gen_opt.zero_grad()\n",
    "    gen_loss.backward()\n",
    "    gen_opt.step()\n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "TENSORBOARD_LOGDIR = \"./logs\"\n",
    "MODEL_NAME = \"DIRCv1\"\n",
    "summary_writer = SummaryWriter(log_dir=os.path.join(TENSORBOARD_LOGDIR, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCRIMINATOR_ITERATIONS_PER_GENEREATOR = 5\n",
    "VALIDATION_INTERVAL = 256\n",
    "HIST_BINS=100\n",
    "DATA_HIST_RANGE=[-5, 5]\n",
    "\n",
    "data_linspace_np = np.linspace(0, 10, num=256, dtype=np.float32)\n",
    "data_linspace_torch = torch.from_numpy(data_linspace_np)[:, None].cuda()\n",
    "validation_data = torch.from_numpy(test).cuda()\n",
    "validation_data_np = test\n",
    "validation_y = validation_data[:, len(X_COLUMNS):]\n",
    "\n",
    "for i in range(10000):\n",
    "    for _ in range(DISCRIMINATOR_ITERATIONS_PER_GENEREATOR):\n",
    "        disc_loss_this_iter = train_disc()\n",
    "\n",
    "    gen_loss_this_iter = train_gen()\n",
    "    gen_scheduler.step()\n",
    "    disc_scheduler.step()\n",
    "    summary_writer.add_scalar(\"discriminator loss\", disc_loss_this_iter,\n",
    "                              global_step=i)\n",
    "    summary_writer.add_scalar(\"generator loss\", gen_loss_this_iter,\n",
    "                              global_step=i)\n",
    "    if i % VALIDATION_INTERVAL == 0:\n",
    "        clear_output(True)\n",
    "        validation_generated = sample_gen_data(validation_y)\n",
    "        validation_generated_np = validation_generated.data.cpu().numpy()\n",
    "\n",
    "        fig, axes_list = plt.subplots(ncols=5, figsize=[6*len(Y_COLUMNS), 6])\n",
    "        for index, ax in enumerate(axes_list):\n",
    "          ax.hist(validation_generated_np[:, index], range=DATA_HIST_RANGE,\n",
    "                  alpha=0.5, density=True, label='Generated', bins=HIST_BINS)\n",
    "          ax.hist(validation_data_np[:, index], range=DATA_HIST_RANGE,\n",
    "                  alpha=0.5, density=True, label='Real', bins=HIST_BINS)\n",
    "          ax.set_xlabel(data.columns[index])\n",
    "          ks_result = ks_2samp(validation_generated_np[:, index],\n",
    "                                    validation_data_np[:, index])\n",
    "          ax.set_title(\"KS stat = {:.4f}; p-value = {:.4E}\".format(*tuple(ks_result)))\n",
    "        fig.suptitle(\"Iteration {}\".format(i))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After a little bit of training, the plots should look like this: ![semitrained GAN](https://github.com/yandexdataschool/mlhep2019/raw/master/notebooks/day-6/semi-trained.png)\n",
    "\n",
    "If you want to push precision to the limit, you want to increase batch size and reduce learning rate over time. You'll also need to monitor the validation losses to avoid overfitting.\n",
    "\n",
    "P. S.\n",
    "As you know from the [lectures](https://en.pelican.study/classroom/213/dialogs/2614/run/), these plots do not comprehensively describe the quality of the GAN. Feel free to add better quality measures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
